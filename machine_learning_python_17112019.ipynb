{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/datamind-dotfit/machine_learning_python/blob/master/machine_learning_python_17112019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwgjhQKBGsGA"
   },
   "source": [
    "# Machine Learning in Python\n",
    "\n",
    "Dit notebook neemt je in secties mee door een inkomensclassificatieproject (drie maal woordwaarde). Op basis van een aantal eigenschappen van personen, zullen we voorspellen of ze meer of minder dan het gemiddelde van de groep verdienen. We doen dat aan de hand van vijf stappen:\n",
    "\n",
    "\n",
    "\n",
    "1.   Data verzamelen\n",
    "2.   Data opschonen en voorbereiden\n",
    "3.   Trainen van het model\n",
    "4.   Testen van het model\n",
    "5.   Verbeteren van een model\n",
    "\n",
    "Aan het einde van de dag heb je praktische vaardigheden in Python opgedaan met alle vijf deze stappen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "seTya9Opcqfm"
   },
   "source": [
    "## 1 Data verzamelen\n",
    "\n",
    "Gisteren is het verzamelen van data uitgebreid aan bod gekomen. We zullen het vandaag daarom niet moeilijk maken: er staat een csv klaar met data en wij hebben al bepaald welke kolommen relevant zijn. Met de read_csv functie van pandas kunnen we dit bestand makkelijk inlezen en klaarmaken voor bewerking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYy6rmDTmU37"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!wget https://github.com/datamind-dotfit/machine_learning_python/blob/master/adult.data?raw=true -O adult.data\n",
    "\n",
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', \n",
    "        'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'label']\n",
    "df = pd.read_csv('adult.data', names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46H1WERoezY_"
   },
   "source": [
    "Om te dubbelchecken of we de juiste data hebben binnengehaald, bekijk je de eerste vijf rijen van de data even met de .head() functie van het dataframe. Alles oke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSl6xiKlmU4H"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XV5_uJ0mU4N"
   },
   "source": [
    "## 2 Data opschonen en voorbereiden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6UPFqftoxH8"
   },
   "source": [
    "### 2.1 Exploratory Data Analysis\n",
    "\n",
    "Het bekijken van een dataframe is erg makkelijk met de pandas profiling tool. Hiermee krijg je een uitgebreide samenvatting per kolom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImFOjyCZiF6Z"
   },
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "report = pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P30qSUgzpu6C"
   },
   "source": [
    "Bekijk het ProfileReport. Vallen er al dingen op? Begrijp je van alle kolommen wat ze betekenen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAxqTMZrpvQi"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om het ProfileReport te bekijken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnHgs4oJp6Nq"
   },
   "source": [
    "Vragen:\n",
    "- Hoeveel missende waardes zijn er? Zijn er zaken die niet direct missen, maar wel als missende waarde kunnen worden aangemerkt?\n",
    "- Kun je meetfouten ontdekken? Zijn er bijvoorbeeld waardes afgekapt?\n",
    "- Bekijk voor alle kolommen de verdeling van de waardes. Welke kolommen zijn normaal verdeeld? En welke hebben een verdeling die in het oog springt? Waarom?\n",
    "\n",
    "We zullen deze vragen klassikaal bespreken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LxAw10Gw_7p"
   },
   "source": [
    "### 2.2 Missende waardes\n",
    "\n",
    "Zoals uitgelegd zijn er verschillende manier om met missende waardes om te gaan. Dataframes ('df' in ons geval) en Series (kolommen zoals df['age']) hebben de fillna() functie. Aan deze functie kun je een waarde meegeven. \n",
    "\n",
    "In ons dataframe zijn geen expliciet missende waardes. Om de werking van de fillna() functie toch duidelijk te maken, zullen we dit oefenen met een klein dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soOImZAOzqBi"
   },
   "outputs": [],
   "source": [
    "df_missing = pd.DataFrame(np.random.randn(10,3))\n",
    "df_missing.iloc[3:5,0] = np.nan\n",
    "df_missing.iloc[4:6,1] = np.nan\n",
    "df_missing.iloc[5:8,2] = np.nan\n",
    "\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYjBKW_l0CT_"
   },
   "source": [
    "Probeer eerst alle missende waardes te vullen met 0. Dat kun je doen door df.fillna(0) te gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eO3kNb6c0L-v"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om de missende waardes in df_missing met 0 te vullen. Maak hiervoor een nieuw dataframe aan genaamd df_missing_zero. \n",
    "# Bekijk het dataframe daarna om te checken of alle waardes goed zijn ingevuld.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6FNCt2i0VVO"
   },
   "source": [
    "Probeer dit nu door de gemiddelde waarde van het dataframe in te vullen in alle missende velden. df_missing.mean() geeft je het gemiddelde van alle waardes per kolom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdeUirE80goo"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om de missende waardes in df_missing met het gemiddelde van hun kolom te vullen. Maak hiervoor een nieuw dataframe aan genaamd df_missing_mean.\n",
    "# Bekijk het dataframe daarna om te checken of alle waardes goed zijn ingevuld.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IdCyqzOv09bB"
   },
   "source": [
    "Een andere manier om de missende waardes te verwerken, is door ze simpelweg te verwijderen. Dit kan uiteraard niet zonder de corresponderende rijen en/of kolommen ook te verwijderen. Hiervoor heeft pandas de functie dropna():\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "\n",
    "Probeer de missende waardes in df_missing te verwijderen. Stel dat we rijen willen behouden als ze 0 of 1 missende waardes hebben. Bij 2 of meer missende waardes willen we ze dus weggooien. \n",
    "\n",
    "Let op: axis=1 voor kolommen en axis=0 voor rijen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKg99LB92Pnn"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om de missende waardes in df_missing te verwijderen aan de hand van de instructie hierboven.\n",
    "# Bekijk het dataframe daarna om te checken of alle waardes goed zijn verwijderd.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pdeXqLq2xPN"
   },
   "source": [
    "Probeer nu hetzelfde voor kolommen die meer dan twee missende waardes hebben. Vul daarna de missende waardes in met het gemiddelde van de kolom en bekijk het resulterende dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2QYWoxJ3BAh"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om de missende waardes in df_missing te verwijderen en in te vullen aan de hand van de instructie hierboven.\n",
    "# Bekijk het dataframe daarna om te checken of alle waardes goed zijn verwijderd en ingevuld.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Ld2nXkxxayW"
   },
   "source": [
    "### 2.3 Uitschieters\n",
    "\n",
    "Uitschieters worden vaak veroorzaakt door meet- of procesfouten, maar zijn soms legitieme metingen van ongebruikelijke waardes. Wat valt je op, kijkend naar bijvoorbeeld de capital_gain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "P_5rN5ubLY6s",
    "outputId": "1623b6f2-089b-4264-cba6-52a7c9cf09d7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df['capital_gain'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "_6cMX-TrNl9E",
    "outputId": "10b1e53d-6f42-477b-9135-ba6e1fc565c4"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om een histogram te maken van de capital_gain kolom voor waardes hoger dan 20000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EW3WYTnANbv9"
   },
   "source": [
    "Er zijn verschillende opties om deze data geschikter te maken voor een machine learning opdracht. Denk aan het weglaten van de metingen of het mathematisch of handmatig transformeren van de metingen. Schrijf hier onder op wat je voorstel is om met deze waardes te doen. We zullen deze opdracht klassikaal bespreken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C50EyZ_0RQ31"
   },
   "outputs": [],
   "source": [
    "# Ik zou de capital gain..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90hawztsxdoe"
   },
   "source": [
    "### 2.4 Ongebalanceerde data\n",
    "\n",
    "Een goed voorbeeld van ongebalanceerde data is (niet geheel toevallig) te vinden in het label in ons dataframe. Bekijk met behulp van de documentatie van de value_counts() functie hoe scheef deze verdeling is:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\n",
    "\n",
    "Het label, of target, wordt gebruikt om het algoritme te leren welk type inkomen hoort bij welke combinatie van eigenschappen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "0-PoE34MRcRN",
    "outputId": "47b5781d-21a2-4b40-9827-cdec8ad5437d"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om te bekijken hoe scheef de verdeling over labels is in onze dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRSJ1RSDUerJ"
   },
   "source": [
    "Zoals je ziet zijn er voor elke persoon met een inkomen hoger dan \\$50.000, wel drie mensen met een inkomen lager dan dat. Door altijd een inkomen lager dan \\$50.000 te voorspellen, zou een model dus al 76% van de tijd de juiste voorspelling doen. In deze sectie zullen we 1 methode verder onder de loep nemen: het samplen van de data. De overige methodes uit de presentatie komen later in de opdrachten of helemaal niet aan bod, maar kunnen soms ook nuttig zijn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxbrxwWYV-5R"
   },
   "source": [
    "#### 2.4.1 Undersampling\n",
    "\n",
    "Door alle waarnemingen uit de minderheidsklasse te kiezen en een gedeelte van de waarnemingen uit de meerderheidsklasse, kunnen we een gebalanceerde dataset creeeren. \n",
    "\n",
    "Met de pandas functie concat() kunnen we meerdere dataframes aan elkaar plakken:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "\n",
    "Probeer nu om alle waarnemingen waarbij het label '>50K' is in een nieuw dataframe genaamd df_balanced onder te brengen. Voeg daarbij een gelijk aantal waarnemingen waarbij het label '<=50K' is. De sample() functie kan daarbij nuttig zijn:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html\n",
    "\n",
    "Let er op dat er wat whitespace om de strings van de labels staat:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.strip.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PZbAZwQXgHZ"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om een gebalanceerde dataset op te bouwen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-g3_gh4OOFS3"
   },
   "source": [
    "#### 2.4.1 Oversampling\n",
    "\n",
    "We kunnen hetzelfde principe gebruiken om de minderheidsklasse vaker te vertegenwoordigen in onze dataset. De sample() functie heeft een mogelijkheid tot samplen 'met terugleggen'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OSVgrLXOhc_"
   },
   "outputs": [],
   "source": [
    "df_balanced_os = df[df['label'].str.strip() == '<=50K']\n",
    "df_balanced_os = pd.concat([df_balanced_os, df[df['label'].str.strip() == '>50K'].sample(df_balanced_os.shape[0])], replace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPbr0ReRxdw0"
   },
   "source": [
    "### 2.5 Transformaties\n",
    "\n",
    "Om het verschil tussen standaardisatie en normalisatie duidelijk te maken, zullen we gestandaardiseerde en genormaliseerde versies van hours_per_week maken. Machine learning library sklearn heeft hier handige tools voor: StandardScaler (standaardisatie) en MinMaxScaler (normalisatie). Deze 'fitten' we eerst op de data, waarna we de scaler kunnen gebruiken om transformaties door te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9YpQ4P4Sc5S"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "standardscaler = StandardScaler()\n",
    "standardscaler.fit(df['hours_per_week'].values.reshape(-1, 1))\n",
    "df['hours_per_week_std'] = standardscaler.transform(df['hours_per_week'].values.reshape(-1, 1))\n",
    "\n",
    "minmaxscaler = MinMaxScaler()\n",
    "minmaxscaler.fit(df['hours_per_week'].values.reshape(-1, 1))\n",
    "df['hours_per_week_nrm'] = minmaxscaler.transform(df['hours_per_week'].values.reshape(-1, 1))\n",
    "\n",
    "df[['hours_per_week', 'hours_per_week_std', 'hours_per_week_nrm']].hist(bins=20)\n",
    "df = df.drop(columns=['hours_per_week_std', 'hours_per_week_nrm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elc9fqjRYHai"
   },
   "source": [
    "Probeer dit nu zelf voor de capital_gain en capital_loss kolommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsuzAByZYdQJ"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om de capital_gain en capital_loss kolommen te transformeren. Kies de variant die jou het nuttigst lijkt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZvX9_2iZ2le"
   },
   "source": [
    "Het omzetten van categorische variabelen naar mathematisch bewerkbare varianten kunnen we doen met de get_dummies() functie van pandas:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "\n",
    "Een relatief nieuw maar zeer belangrijk argument voor deze functie is 'drop_first'.\n",
    "\n",
    "Laten we eerst bekijken voor welke kolommen deze operatie nodig is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Db9XOKIkaMip"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om te bekijken welke kolommen categorische waarden hebben. \n",
    "# Sla de namen van de categorische kolommen op in een variabele categorical_column.\n",
    "# Sla ook de namen van de overige kolommen op ine en aparte variabele genaamd numerical_column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDl2XDyebnBL"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om voor alle categorische kolommen de dummykolommen aan te maken en toe \n",
    "# te voegen aan een nieuw dataframe genaamd df_onehot. Verwijder de originele kolommen uit het dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27inEMqLmU4a"
   },
   "source": [
    "### 2.6 Bonus: Trainset maken\n",
    "\n",
    "Met het afronden van alle bovenstaande stappen, wordt het tijd om de trainset op te bouwen. Maak een dataframe waarin:\n",
    "\n",
    " * uitschieters op een zinnige manier verwerkt zijn\n",
    " * alle categorische variabelen one-hot encoded zijn\n",
    " * alle numerieke variabelen gestandaardiseerd zijn \n",
    " * het label binair is en de naam 'label' heeft\n",
    " * even veel positieve als negatieve targets voorkomen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVY2wMlzhBTw"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om een trainset te maken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62jJhesimU41"
   },
   "source": [
    "## 3 Opzetten van een experiment en trainen van het model\n",
    "\n",
    "Met een kant en klare trainset kunnen we aan de slag om het experiment voor te bereiden. Mocht je de bonusopdracht uit het vorige deel niet hebben afgerond, dan kun je de door ons voorbereide trainset zo binnenhalen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT0Ur1M-pGow"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/datamind-dotfit/machine_learning_python/blob/master/train.csv?raw=true -O train.csv\n",
    "data = pd.read_csv('train.csv')\n",
    "data = data.drop(data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6x0bKbaqGF7"
   },
   "source": [
    "### 3.1 Opzetten van een experiment\n",
    "\n",
    "Om een gedegen experiment te doen, bepalen we vooraf vijf zaken:\n",
    "\n",
    "\n",
    "\n",
    "1.   Ons target (wat willen we voorspellen)\n",
    "2.   De evaluatiemethode (hoe bepalen we hoe goed ons model presteert)\n",
    "3.   De baselineprestatie (welke performance kunnen we minimaal verwachten)\n",
    "4.   Een validatietechniek (hoe zorgen we dat we de performance eerlijk beoordelen)\n",
    "5.   Test- en validatiedataset (hoe verdelen we onze waarnemingen over onze datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nndCSr0EmU42"
   },
   "source": [
    "#### 3.1.1 Target\n",
    "We willen voorspellen of personen meer of minder dan \\$50.000 per jaar verdienen. In ons dataframe heet deze kolom 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPVqOhypmU42"
   },
   "outputs": [],
   "source": [
    "target = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXWe2VfemU47"
   },
   "source": [
    "#### 3.1.2 Evaluatiemethode\n",
    "\n",
    "Om te bepalen hoe goed het model presteert, moeten we kwantificeren wat 'goed' betekent. In ons geval zijn we geinteresseerd in het correct classificeren van inkomen. Als eerste metric kunnen we daarvoor accuracy gebruiken. Andere mogelijk interessante metrics zijn precision, recall, de F1-score of de AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gb6Q3Oi4mU49"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zes7EocVmU5A"
   },
   "source": [
    "#### 3.1.3 Baselineprestatie\n",
    "\n",
    "De eenvoudigste baselineprestatie is het voorspellen van de meerderheidsklasse. In dit geval voorspellen we dan 76% van de gevallen goed (<=50K per jaar). Met deze baseline kunnen we werken naar een elegantere oplossing, die vaker de juiste inkomensklasse voorspelt. Dat kan soms op basis van bijvoorbeeld business ruling, maar wij zullen daar een model voor maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7f_Cq8yvm2l"
   },
   "outputs": [],
   "source": [
    "# Meerderheidsklasse berekenen\n",
    "majority_class = # TO DO \n",
    "print(f\"Majority class is {majority_class}\")\n",
    "\n",
    "# Accuracy berekenen\n",
    "from sklearn.metrics import accuracy_score\n",
    "df['baseline'] = majority_class\n",
    "baseline_accuracy = accuracy_score(df['label'], df['baseline'])\n",
    "\n",
    "print(f\"Baseline accuracy is: {baseline_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZzaHWiOmU5G"
   },
   "source": [
    "#### 3.1.4 Validatietechniek\n",
    "Het is belangrijk om de prestaties van een model op correcte wijze te evalueren. Trainen en testen op dezelfde data is niet toegestaan. Als je dit wel doet, loop je het risico dat je model niet heeft geleerd te generaliseren, wat wel het doel is van een model. \n",
    "\n",
    "Hoe we dit bepalen is door een validatie- en testset apart te houden tijdens de trainingsprocedure. Het trainen doen we op de trainset - simpel. Daarna proberen we op de validatieset uit of het model nog fijner geslepen kan worden. In de regel gaat het hier om optimaliseren van modelparameters. Op de testset wordt tot slot de performance bepaald om te zien hoe het model in de echte wereld zou presteren.\n",
    "\n",
    "Om dit nog beter te beoordelen, wordt vaak K-fold cross validation gebruikt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD88TIZ4mU5K"
   },
   "source": [
    "#### 3.1.5 Validatie- en testset maken\n",
    "Een validatie- en testset maken kan met de hand of door gebruik te maken van de tools in sklearn. Een standaard ratio om data te verdelen is 80% train, 10% validatie en 10% test. \n",
    "\n",
    "Let er op dat je de data uniform verdeelt over deze sets. Je wilt bijvoorbeeld niet dat alle voorbeelden uit de eerste maand van je data in de trainset zitten, terwijl alle latere voorbeelden in je testset terecht komen. Ditzelfde geldt voor zaken als de verdeling van targets over de sets - een testset zonder positieve targets is niet representatief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SB0Vbs4LmU5M"
   },
   "outputs": [],
   "source": [
    "# Train / validatie / testsets maken\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, val_test = train_test_split(data, train_size=0.8)\n",
    "val, test = train_test_split(val_test, train_size=0.5)\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cL2o2X7swspd"
   },
   "source": [
    "## 3.2 Trainen van een model\n",
    "\n",
    "Nu we onze data en het experiment hebben voorbereid, kunnen we modellen gaan trainen. We zullen twee algoritmes gebruiken: Logistische regressie en random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTk-hPkMmU5i"
   },
   "source": [
    "### 3.2.1 Logistische regressie\n",
    "Logistische regressie, in bepaalde kringen ook bekend als 'proc logistic', doet een aantal aannames over de onderliggende data. \n",
    "\n",
    "Vragen:\n",
    "\n",
    "*   Welke aannames doet een logistische regressie over de onderliggende data?\n",
    "*   In welke mate voldoet onze dataset daar aan?\n",
    "\n",
    "Gebruik de logistische regressie module van sklearn om het volgende te doen:\n",
    " - importeren van de module\n",
    " - splitsen van de kolommen in de dataset in features en target\n",
    " - aanmaken van een LogisticRegression object\n",
    " - de trainset gebruiken om het regressiemodel te schatten\n",
    "\n",
    " Zoals altijd is de documentatie van cruciale waarde:\n",
    "\n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XppWdnNnmU5j"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om de regressiemodule te importeren, kolommen in de dataset te splitsen, \n",
    "# het regressieobject aan te maken en te trainen op de trainset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9J7x2p6mU5o"
   },
   "source": [
    "### 3.2.2 Random forest\n",
    "\n",
    "Een random forest is een collectie beslisbomen, die samen tot een voorspelling komen. Het 'random' deel in de naam random forest slaat op hoe er delen van bomen willekeurig worden geselecteerd. Omdat beslisbomen niet-lineaire verbanden goed kunnen voorspellen, kan het collectief dat ook. \n",
    "\n",
    "Vraag:\n",
    "\n",
    "* In welke mate zijn de vereisten die door logistische regressie aan de data worden gesteld van toepassing bij gebruik van een random forest? \n",
    "\n",
    "Gebruik de random forest module van sklearn om het volgende te doen:\n",
    " - importeren van de module\n",
    " - aanmaken van een RandomForest object\n",
    " - de trainset gebruiken om het regressiemodel te schatten\n",
    " - berekenen van de accuracy van het model op de validatieset\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzWzrAAhmU5o"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code voor het maken en trainen van een random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkMubdTY6v2g"
   },
   "source": [
    "## 4 Testen van een model\n",
    "\n",
    "We hebben nu twee getrainde modellen. We kunnen de performance vergelijken op basis van onze eerder gekozen performance metric. Daarnaast kunnen we K-fold cross validatie doen voor een nog accurater beeld van onze prestaties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5FOtFf-O6d5B"
   },
   "source": [
    " Drie opdrachten:\n",
    " - bereken de logistiche regressie accuracy op de validatieset\n",
    " - bereken de random forest accuracy op de validatieset\n",
    " - neem de trainset en bereken de 5 fold cross validatie score (https://scikit-learn.org/stable/modules/cross_validation.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUR-LvdxmU5m"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om de accuracy van de logistische regressie te bepalen.\n",
    "# Sla daarvoor de voorspellingen van het model op de validatieset op en vergelijk ze met de daadwerkelijke waardes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fYymUMemU5q"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code voor het evalueren van de performance van het random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3B43RMgA931"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code voor het berekenen van de 5 fold cross validatie score voor de twee modellen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcky3GZXD08n"
   },
   "source": [
    "Vragen:\n",
    "* Hoe verhouden de 5 fold cross validation en de accuracy op de afzonderlijke modellen zich tot elkaar? Op welke sets worden ze berekend? \n",
    "* Welke maat is betrouwbaarder?\n",
    "* Hoe verhouden de scores zich tot de baseline die we hebben vastgesteld?\n",
    "* Welk model is beter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve\n",
    "Om beter inzicht te krijgen in hoe ons model presteert ten op zichte van andere modellen, kunnen we een ROC Curve maken. Op deze curve wordt de True Positive Rate uitgezet tegen de False Positive Rate voor een bepaalde probability threshold. \n",
    "\n",
    "<img src=\"https://ars.els-cdn.com/content/image/3-s2.0-B9780123756725100027-f02-02-9780123756725.jpg\">\n",
    "\n",
    "In de ROC curve zien we vaak ook een diagonale lijn, deze geeft aan dat in verhouding evenveel true positives false positives worden gemaakt. Dit wordt ook wel de random lijn genoemd, de prestaties zijn dan vergelijkbaar met het gooien van een muntje.\n",
    "\n",
    "Hoe meer de curve naar links boven gaat, hoe meer true positives we maken per false positive en dus een beter model. \n",
    "\n",
    "Als we denken aan een classificatieprobleem dan kunnen we een model een 1 of een 0 terug laten geven of een kans. Met deze kans kan vervolgens de ROC Curve worden berekend. Om de kans te pakken gaan we de *predict_proba* gebruiken in plaats van de gewone predict functie. Dit geeft de kansverdeling op een 0 en een 1 per voorbeeld.\n",
    "<br>\n",
    "## Oefening\n",
    "Doorloop de volgende stappen voor ieder model:\n",
    "- Maak een cel aan en sla hier je de **y** van de validation set op in een herkenbare variabele\n",
    "- Definineer in een nieuwe cel de validatie features die je gaat voorspellen.\n",
    "- Roep de predict_proba functie aan op je model en sla de resultaten op in herkenbare variabele.\n",
    "- Roep de functie draw_roc_curve aan met de juiste parameters en bekijk de plot. Vergelijk de plot ook met de prestaties van een ander model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De werkelijke classificatie van de validatie data\n",
    "ground_truth = # To do\n",
    "print(ground_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validatie features\n",
    "val_features = # To do \n",
    "print(val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorspel de probabilities\n",
    "model_probs = # To do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deze code is al voorgemaakt en helpt bij het plotten\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw_roc_curve(ground_truth, predicted_probabilities):\n",
    "    if predicted_probabilities.shape[1] == 2:\n",
    "    predicted_probabilities = predicted_probabilities[:, 1]\n",
    "    ns_probs = [1 for _ in range(len(ground_truth))]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(ground_truth, ns_probs)\n",
    "    lr_auc = roc_auc_score(ground_truth, predicted_probabilities)\n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print('Model: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(ground_truth, ns_probs)\n",
    "    model_fpr, model_tpr, _ = roc_curve(ground_truth, predicted_probabilities)\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    pyplot.plot(model_fpr, model_tpr, marker='.', label='Model')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roep hier de plot_roc_curve functie aan met de juiste parameters\n",
    "# To do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roep hier de plot_roc_curve functie aan voor een tweede model\n",
    "# To do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLyhI5D0dlfA"
   },
   "source": [
    "## 5 Een model verbeteren\n",
    "\n",
    "Het verbeteren van de performance van een model kan op verschillende manieren. Omdat het toevoegen van meer en betere data bijna altijd een verbetering oplevert in de performance, is dit zonder twijfel de meest gewenste oplossing. Helaas is dit vaak ook de minst makkelijk te realiseren oplossing. \n",
    "\n",
    "In ons geval is het onmogelijk om meer data te verkrijgen. Wel kunnen we experimenteren met andere algoritmes en nieuwe features uit de data creeeren. Vooral dat laatste is interessant als je de performance van een model wilt verbeteren. Hier komt vaak wel wat domeinkennis bij kijken, die niet altijd voorhanden is bij de data scientist zelf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJ3WbszVqnYe"
   },
   "source": [
    "### 5.1 Andere algoritmes\n",
    "\n",
    "Het is interessant om met verschillende types algoritme te experimenteren. Soms zul je vinden dat een probleem erg geschikt is voor het gebruik van een lineaire voorspeller. In andere gevallen zijn de relaties tussen features en target niet-lineair en werkt een niet-lineaire voorspeller (zoals random forest of een deep learning aanpak) beter.\n",
    "\n",
    "Boosting is een methode waarbij een set van voorspellers wordt gebruikt om tot een accurate voorspelling te komen. Aan de correct geclassificeerde samples van 1 voorspeller wordt een lager gewicht toegekend dan aan de incorrect geclassificeerde. Vervolgens wordt een nieuwe voorspeller getraind die beter is in het classificeren van de fouten van de eerste. Dit proces wordt een aantal keer herhaald, tot er een set van voorspellers is die samen een goede voorspelling kan maken. Het concept van boosting en een specifieke implementatie ervan, Xtreme Gradient Boosting, worden op dit moment erg veel gebruikt vanwege de goede performance. Niet alleen voorspellen modellen vaak erg goed, maar ook is het algoritme erg efficient waardoor je makkelijk en snel modellen kunt trainen.\n",
    "\n",
    "Probeer nu met de GradientBoostingClassifier van sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) of je betere performance kunt krijgen dan met onze eerder gemaakte modellen.\n",
    "\n",
    "Schroom niet om verschillende instellingen van het algoritme te proberen. Het tunen van hyperparameters is een belangrijke methode om de performance van een model nog verder te verbeteren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X90aDiaewDpW"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om een GradientBoostingClassifier te trainen en te evalueren.\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "gbc.fit(# TO DO )\n",
    "y_hat_gbc = # TO DO\n",
    "\n",
    "\n",
    "gradient_boosting_accuracy = accuracy_score(val[target], y_hat_gbc)\n",
    "print(f\"Gradient Boosting accuracy is: {gradient_boosting_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0u1KTtagqnlK"
   },
   "source": [
    "### 5.2 Feature engineering\n",
    "\n",
    "De meeste winst valt meestal te halen in het bewerken van de data tot nieuwe features. De domeinkennis die hiervoor nodig is maakt dit echter niet altijd triviaal. We zullen proberen om in onze dataset nieuwe, gecombineerde features te maken die tot betere voorspellingen leiden.\n",
    "\n",
    "Om te beginnen is het een goed idee om te kijken welke features nu al belangrijk zijn in het model. Dit is eenvoudig te doen voor ons getrainde random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "colab_type": "code",
    "id": "QugMD2w9mU5s",
    "outputId": "a55ed6f9-1c4c-4dd2-d640-921663d21d57"
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(data=list(zip(variables, clf.feature_importances_)), columns=['var', 'importance']).sort_values('importance', ascending=False)\n",
    "feature_importances.set_index('var').nlargest(12, columns=['importance']).plot(kind='barh', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6AtQCwR0PIg"
   },
   "source": [
    "Niet onverwacht zijn de leeftijd, opleiding, aantal personen in het huishouden en uren per week dat er wordt gewerkt belangrijk voor de voorspelling. Het is redelijk om aan te nemen dat het aantal uren dat iemand werkt gecombineerd met zijn opleidingsniveau, een correlatie heeft met zijn inkomen. We zullen dit testen door een samengestelde feature te maken uit deze twee features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "DFlagatv2Q-s",
    "outputId": "40375d95-68ea-4f08-ddf4-3eff54fd3c8e"
   },
   "outputs": [],
   "source": [
    "# Schrijf hier code om een nieuwe feature genaamd 'hpw_x_education' te maken, die wordt berekend door de hours per week te vermenigvuldigen met de education_num.\n",
    "# Train daarna een model op deze nieuwe dataset en evalueer de performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nR2FqCD2wdU"
   },
   "source": [
    "Vragen:\n",
    "\n",
    "* Wat doe je met de originele features nadat je de nieuwe samengestelde feature hebt gemaakt?\n",
    "* Moet je de nieuwe feature schalen?\n",
    "* Hoe leuk was het om de hele dataset opnieuw te definieren en het voorspel- en evaluatieproces opnieuw te doorlopen?\n",
    "\n",
    "Op de laatste vraag zullen we klassikaal terugkomen."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "machine_learning_python_17112019.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
